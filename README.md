# ðŸ§  Darija LLM â€“ Training a Custom Language Model on WhatsApp Chat Data

This project demonstrates how to build and train a small Large Language Model (LLM) using **WhatsApp-style chat data** in a low-resource language â€” in this case, **Darija**, the Moroccan Arabic dialect.

Although Iâ€™m an Indian developer and donâ€™t speak Darija, I chose this dataset intentionally to explore how transformer models behave when trained on underrepresented, conversational languages. The techniques shown here apply equally well to **Hinglish, Hindi, Marathi**, and other code-mixed or Indian vernaculars.

This project serves as a practical guide for anyone looking to create their own LLM â€” from scratch â€” in any language.

---

## ðŸ“š Project Highlights

- âœ… WhatsApp-style chat dataset used for training
- âœ… End-to-end training: from tokenization to fine-tuning
- âœ… Custom GPT-style Transformer model
- âœ… LoRA-based instruction tuning
- âœ… Lightweight and fast training (~42M parameters)
- âœ… Adaptable to **any language** (e.g., Hinglish, Hindi, Bengali)

---

## ðŸ“¦ Repository Structure

```bash
.
â”œâ”€â”€ notebooks/             # Jupyter notebooks for each pipeline stage
â”œâ”€â”€ transformer/           # Custom GPT-style transformer code
â”œâ”€â”€ minbpe/                # Minimal BPE tokenizer
â”œâ”€â”€ data/                  # Training data (Darija chat samples)
â”œâ”€â”€ assets/                # Chat samples, screenshots
â”œâ”€â”€ Slides.odp             # Slides used in video walkthrough
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # This file
